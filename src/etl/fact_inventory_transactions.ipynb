{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5fa49dc-b0d5-417c-b33c-767205928234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lakebase_utils import LakebaseConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"user\", \"lars.liahagen@databricks.com\")\n",
    "username = dbutils.widgets.get(\"user\")\n",
    "dbutils.widgets.text(\"lakebase_instance_name\", \"smart-stock-db\")\n",
    "lakebase_instance_name = dbutils.widgets.get(\"lakebase_instance_name\")\n",
    "dbutils.widgets.text(\"catalog\", \"smart_stock\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "dbutils.widgets.text(\"schema_silver\", \"smart_stock_silver\")\n",
    "schema_silver = dbutils.widgets.get(\"schema_silver\")\n",
    "dbutils.widgets.text(\"schema_gold\", \"smart_stock_gold\")\n",
    "schema_gold = dbutils.widgets.get(\"schema_gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2a7094-83d8-4e6b-a59c-cc00ffcb7358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# CONFIGURATION\n",
    "# =============================================\n",
    "\n",
    "# PostgreSQL connection\n",
    "conn = LakebaseConnection(username, lakebase_instance_name)\n",
    "\n",
    "print(\"✅ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9878b1-f0db-4071-b62b-7b4c19f42c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `{catalog}`.`{schema_gold}`.fact_inventory_transactions (\n",
    "    transaction_id BIGINT,\n",
    "    transaction_number STRING,\n",
    "    transaction_type STRING,\n",
    "    status STRING,\n",
    "    quantity_change INT,\n",
    "    notes STRING,\n",
    "    transaction_timestamp TIMESTAMP,\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    category STRING,\n",
    "    unit_price DECIMAL(10,2),\n",
    "    warehouse_id INT,\n",
    "    warehouse_name STRING,\n",
    "    warehouse_location STRING,\n",
    "    transaction_date DATE,\n",
    "    year INT,\n",
    "    month INT,\n",
    "    week INT,\n",
    "    day_of_week INT,\n",
    "    hour INT,\n",
    "    inbound_quantity INT,\n",
    "    outbound_quantity INT,\n",
    "    adjustment_quantity INT,\n",
    "    transaction_value DECIMAL(12,2),\n",
    "    is_pending INT,\n",
    "    is_delivered INT,\n",
    "    is_urgent INT,\n",
    "    processed_at TIMESTAMP\n",
    ")\n",
    "CLUSTER BY (transaction_timestamp)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685046ad-99af-432e-8c01-34da8a5f62ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_last_transaction_timestamp():\n",
    "    \"\"\"Get the timestamp of the last processed transaction\"\"\"\n",
    "    try:\n",
    "        last_ts = spark.sql(f\"\"\"\n",
    "            SELECT MAX(transaction_timestamp) as max_ts \n",
    "            FROM `{catalog}`.`{schema_gold}`.fact_inventory_transactions\n",
    "        \"\"\").collect()[0]['max_ts']\n",
    "        return last_ts if last_ts else \"2024-01-01 00:00:00\"\n",
    "    except:\n",
    "        return \"1970-01-01 00:00:00\"  # First run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_critical_stock(new_transactions_df):\n",
    "    \"\"\"Check if any new transactions caused critical stock levels\"\"\"\n",
    "    \n",
    "    # Only check for outbound transactions\n",
    "    outbound = new_transactions_df.filter(\n",
    "        (col(\"transaction_type\") == \"sale\") & \n",
    "        (col(\"status\").isin(\"confirmed\", \"processing\", \"pending\"))\n",
    "    )\n",
    "    \n",
    "    if outbound.count() > 0:\n",
    "        # Calculate current stock for affected products\n",
    "        critical_check = spark.sql(f\"\"\"\n",
    "            WITH current_stock AS (\n",
    "                SELECT \n",
    "                    product_id,\n",
    "                    warehouse_id,\n",
    "                    SUM(quantity_change) as stock_level\n",
    "                FROM `{catalog}`.`{schema_gold}`.fact_inventory_transactions\n",
    "                WHERE status IN ('delivered', 'confirmed', 'processing')\n",
    "                GROUP BY product_id, warehouse_id\n",
    "            )\n",
    "            SELECT \n",
    "                cs.product_id,\n",
    "                p.product_name,\n",
    "                cs.warehouse_id,\n",
    "                w.warehouse_name,\n",
    "                cs.stock_level,\n",
    "                p.reorder_level,\n",
    "                CASE \n",
    "                    WHEN cs.stock_level <= 0 THEN 'OUT_OF_STOCK'\n",
    "                    WHEN cs.stock_level <= p.reorder_level THEN 'CRITICAL'\n",
    "                    WHEN cs.stock_level <= p.reorder_level * 1.5 THEN 'LOW'\n",
    "                    ELSE 'OK'\n",
    "                END as stock_status\n",
    "            FROM current_stock cs\n",
    "            JOIN `{catalog}`.`{schema_silver}`.dim_products p ON cs.product_id = p.product_id\n",
    "            JOIN (SELECT DISTINCT warehouse_id, warehouse_name FROM `{catalog}`.`{schema_gold}`.fact_inventory_transactions) w \n",
    "                ON cs.warehouse_id = w.warehouse_id\n",
    "            WHERE cs.stock_level <= p.reorder_level\n",
    "        \"\"\")\n",
    "        \n",
    "        critical_count = critical_check.count()\n",
    "        if critical_count > 0:\n",
    "            print(f\"\\n  ⚠️ ALERT: {critical_count} products at critical levels:\")\n",
    "            critical_check.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d3a5a3-7d31-48a1-92b2-ae29b5f48587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_new_transactions():\n",
    "    \"\"\"\n",
    "    Process new inventory transactions - Run every 1-5 minutes\n",
    "    Only processes new records since last run\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 Checking for new transactions at {datetime.now()}\")\n",
    "    \n",
    "    # Get last processed timestamp\n",
    "    last_timestamp = get_last_transaction_timestamp()\n",
    "    print(f\"  Last processed: {last_timestamp}\")\n",
    "    \n",
    "    # Query only new transactions\n",
    "    new_transactions_query = f\"\"\"\n",
    "        SELECT \n",
    "            CAST(t.transaction_id AS BIGINT) as transaction_id,\n",
    "            CAST(t.transaction_number AS VARCHAR(50)) as transaction_number,\n",
    "            CAST(t.transaction_type AS VARCHAR(50)) as transaction_type,\n",
    "            CAST(t.status AS VARCHAR(20)) as status,\n",
    "            CAST(t.quantity_change AS INTEGER) as quantity_change,\n",
    "            CAST(COALESCE(t.notes, '') AS TEXT) as notes,\n",
    "            t.transaction_timestamp,\n",
    "            CAST(t.product_id AS INTEGER) as product_id,\n",
    "            CAST(p.name AS VARCHAR(200)) as product_name,\n",
    "            CAST(p.category AS VARCHAR(50)) as category,\n",
    "            CAST(p.price AS DECIMAL(10,2)) as price,\n",
    "            CAST(t.warehouse_id AS INTEGER) as warehouse_id,\n",
    "            CAST(w.name AS VARCHAR(100)) as warehouse_name,\n",
    "            CAST(w.location AS VARCHAR(200)) as warehouse_location\n",
    "        FROM inventory_transactions t\n",
    "        JOIN products p ON t.product_id = p.product_id\n",
    "        JOIN warehouses w ON t.warehouse_id = w.warehouse_id\n",
    "        WHERE t.transaction_timestamp > '{last_timestamp}'\n",
    "        ORDER BY t.transaction_timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    new_transactions = conn.execute_query(new_transactions_query)\n",
    "    if new_transactions.empty:\n",
    "        print(\"  ℹ️ No new transactions\")\n",
    "        return 0\n",
    "    \n",
    "    new_transactions = spark.createDataFrame(new_transactions)\n",
    "    \n",
    "    transaction_count = new_transactions.count()\n",
    "    \n",
    "    if transaction_count > 0:\n",
    "        print(f\"  🆕 Found {transaction_count} new transactions\")\n",
    "        \n",
    "        # Enrich and transform\n",
    "        fact_transactions = new_transactions.select(\n",
    "            # Core transaction data with explicit casting\n",
    "            col(\"transaction_id\").cast(\"bigint\"),\n",
    "            col(\"transaction_number\").cast(\"string\"),\n",
    "            col(\"transaction_type\").cast(\"string\"),\n",
    "            col(\"status\").cast(\"string\"),\n",
    "            col(\"quantity_change\").cast(\"int\"),\n",
    "            col(\"notes\").cast(\"string\"),\n",
    "            col(\"transaction_timestamp\").cast(\"timestamp\"),\n",
    "            \n",
    "            # Product dimensions with casting\n",
    "            col(\"product_id\").cast(\"int\"),\n",
    "            col(\"product_name\").cast(\"string\"),\n",
    "            col(\"category\").cast(\"string\"),\n",
    "            col(\"price\").cast(\"decimal(10,2)\").alias(\"unit_price\"),\n",
    "            \n",
    "            # Warehouse dimensions with casting\n",
    "            col(\"warehouse_id\").cast(\"int\"),\n",
    "            col(\"warehouse_name\").cast(\"string\"),\n",
    "            col(\"warehouse_location\").cast(\"string\"),\n",
    "            \n",
    "            # Time dimensions for easy aggregation\n",
    "            date_format(col(\"transaction_timestamp\"), \"yyyy-MM-dd\").cast(\"date\").alias(\"transaction_date\"),\n",
    "            year(col(\"transaction_timestamp\")).cast(\"int\").alias(\"year\"),\n",
    "            month(col(\"transaction_timestamp\")).cast(\"int\").alias(\"month\"),\n",
    "            weekofyear(col(\"transaction_timestamp\")).cast(\"int\").alias(\"week\"),\n",
    "            dayofweek(col(\"transaction_timestamp\")).cast(\"int\").alias(\"day_of_week\"),\n",
    "            hour(col(\"transaction_timestamp\")).cast(\"int\").alias(\"hour\"),\n",
    "            \n",
    "            # Calculated fields with proper casting\n",
    "            when(col(\"transaction_type\") == \"inbound\", col(\"quantity_change\")) \\\n",
    "                .otherwise(0).cast(\"int\").alias(\"inbound_quantity\"),\n",
    "            when(col(\"transaction_type\") == \"sale\", abs(col(\"quantity_change\"))) \\\n",
    "                .otherwise(0).cast(\"int\").alias(\"outbound_quantity\"),\n",
    "            when(col(\"transaction_type\") == \"adjustment\", col(\"quantity_change\")) \\\n",
    "                .otherwise(0).cast(\"int\").alias(\"adjustment_quantity\"),\n",
    "            (abs(col(\"quantity_change\")).cast(\"decimal(10,2)\") * col(\"price\").cast(\"decimal(10,2)\")) \\\n",
    "                .cast(\"decimal(12,2)\").alias(\"transaction_value\"),\n",
    "            \n",
    "            # Flags for filtering (cast as int for consistency)\n",
    "            when(col(\"status\") == \"pending\", 1).otherwise(0).cast(\"int\").alias(\"is_pending\"),\n",
    "            when(col(\"status\") == \"delivered\", 1).otherwise(0).cast(\"int\").alias(\"is_delivered\"),\n",
    "            when(col(\"notes\").contains(\"URGENT\"), 1).otherwise(0).cast(\"int\").alias(\"is_urgent\"),\n",
    "            \n",
    "            # ETL metadata\n",
    "            current_timestamp().alias(\"processed_at\")\n",
    "        )\n",
    "\n",
    "        # Ensure schema matches exactly before writing\n",
    "        # This is critical to avoid merge errors\n",
    "        expected_schema = spark.table(f\"`{catalog}`.`{schema_gold}`.fact_inventory_transactions\").schema\n",
    "        \n",
    "        # Reorder columns to match existing table schema\n",
    "        fact_transactions_final = fact_transactions.select(\n",
    "            [col(field.name).cast(field.dataType) for field in expected_schema.fields]\n",
    "        )\n",
    "        \n",
    "        # Append to fact table\n",
    "        fact_transactions.write \\\n",
    "            .mode(\"append\") \\\n",
    "            .saveAsTable(f\"`{catalog}`.`{schema_gold}`.fact_inventory_transactions\")\n",
    "        \n",
    "        print(f\"✅ Added {transaction_count} new transactions to fact table\")\n",
    "        \n",
    "        # Check for critical alerts\n",
    "        check_critical_stock(new_transactions)\n",
    "        \n",
    "    else:\n",
    "        print(\"  ℹ️ No new transactions\")\n",
    "    \n",
    "    return transaction_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550f0d8c-4a00-4ffb-bf2f-7ffc1d36d913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Checking for new transactions at 2025-09-16 15:58:19.958686\n",
      "  Last processed: 2024-01-01 00:00:00\n",
      "  🆕 Found 98 new transactions\n",
      "✅ Added 98 new transactions to fact table\n",
      "\n",
      "  ⚠️ ALERT: 4 products at critical levels:\n",
      "+----------+--------------------+------------+---------------------------+-----------+-------------+------------+\n",
      "|product_id|product_name        |warehouse_id|warehouse_name             |stock_level|reorder_level|stock_status|\n",
      "+----------+--------------------+------------+---------------------------+-----------+-------------+------------+\n",
      "|38        |Kickstand Heavy Duty|2           |Hamburg Distribution Center|-2         |60           |OUT_OF_STOCK|\n",
      "|29        |Chain 11-Speed      |1           |Lyon Main Warehouse        |5          |50           |CRITICAL    |\n",
      "|17        |Tire 29x2.4 MTB     |1           |Lyon Main Warehouse        |8          |60           |CRITICAL    |\n",
      "|21        |Brake Rotor 180mm   |1           |Lyon Main Warehouse        |-3         |100          |OUT_OF_STOCK|\n",
      "+----------+--------------------+------------+---------------------------+-----------+-------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_new_transactions()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "fact_inventory_transactions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
