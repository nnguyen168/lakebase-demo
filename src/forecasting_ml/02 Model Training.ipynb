{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66d5421f-16ed-4ac6-ba5f-ce9bcf57f7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Wrap-up: Model Logging with Signature Added to MLflow\n",
    "\n",
    "- The MLflow logging process for all warehouse models has been updated to include a model signature, which specifies the expected input and output schema for each model.\n",
    "- The signature is inferred using `mlflow.models.infer_signature` based on a realistic example input and the model's output, ensuring robust schema enforcement.\n",
    "- All models are now logged with their signature, improving reproducibility and making downstream usage (e.g., in batch scoring or model serving) more reliable.\n",
    "- **Warnings:**\n",
    "  * MLflow issued a warning about integer columns and missing values. If your data may contain missing values for integer columns (e.g., `product_id`, `n_periods`), consider using float types or ensuring your input data is always complete.\n",
    "  * MLflow also recommends providing an `input_example` when logging models for better signature validation and UI experience.\n",
    "- Next steps (optional):\n",
    "  * Update the model logging to include an `input_example` for even better documentation and validation.\n",
    "  * Review the MLflow UI to confirm the signature is visible for each logged model.\n",
    "\n",
    "**All requested changes are complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75c6828c-dd47-4840-85df-62edf466bdaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow-skinny=3.3.2\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "076e8fe3-363c-43bf-bc9d-2ef24f85c2b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col\n",
    "import mlflow.pyfunc\n",
    "import mlflow\n",
    "import tempfile\n",
    "import uuid\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e7802f-3525-4b5b-a486-6e36a0518c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"smart_stock\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "dbutils.widgets.text(\"schema_silver\", \"silver\")\n",
    "schema_silver = dbutils.widgets.get(\"schema_silver\")\n",
    "dbutils.widgets.text(\"schema_forecast\", \"forecast\")\n",
    "schema_forecast = dbutils.widgets.get(\"schema_forecast\")\n",
    "dbutils.widgets.text(\"ml_artifact_volume\", \"ml_experiments\")\n",
    "ml_artifact_volume = dbutils.widgets.get(\"ml_artifact_volume\")\n",
    "\n",
    "experiment_name = \"/Shared/smart_stock_sales_forecasting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1848fa67-cfda-42dd-afdc-2321c455e956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Step 1: Prepare Data for Modeling (last 3 years, all warehouse/product pairs)\n",
    "We'll load the last 3 years of data from the sales_history table, identify all unique (warehouse_id, product_id) pairs, and organize the data for modeling. We'll use Spark for efficient data handling, then convert to Pandas for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4384e493-6024-4fca-8ff5-9995bc9a1a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the time window: last 3 years from today\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=3*365)\n",
    "\n",
    "# Load the sales_history table\n",
    "sales_history = spark.table(f\"{catalog}.{schema_silver}.sales_history\")\n",
    "\n",
    "# Filter for last 3 years\n",
    "sales_history_recent = sales_history.filter(col(\"week_start\").between(start_date, end_date))\n",
    "\n",
    "# Identify all unique (warehouse_id, product_id) pairs\n",
    "unique_pairs = (sales_history_recent\n",
    "    .select(\"warehouse_id\", \"product_id\")\n",
    "    .distinct()\n",
    "    .toPandas()\n",
    "    .values.tolist()\n",
    ")\n",
    "\n",
    "print(f\"Number of unique (warehouse_id, product_id) pairs: {len(unique_pairs)}\")\n",
    "display(sales_history_recent.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05d975e5-3327-459e-8c45-1d8ab69c444c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Step 2: Train Simple Forecasting Models for Each (warehouse_id, product_id) Pair\n",
    "We'll iterate over each (warehouse_id, product_id) pair, extract its time series, train an Exponential Smoothing model, store the model, and collect metrics and plots. We'll organize models by warehouse for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e895eee-021e-4bc4-adec-250dfc2c6795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"tmp\", exist_ok=True)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for modeling\n",
    "pdf = sales_history_recent.toPandas()\n",
    "\n",
    "# Ensure week_start is datetime\n",
    "pdf['week_start'] = pd.to_datetime(pdf['week_start'])\n",
    "\n",
    "# Prepare storage\n",
    "warehouse_models = {}\n",
    "warehouse_metrics = {}\n",
    "warehouse_plots = {}\n",
    "product_train_ranges = {}  # Store training date range for each product\n",
    "\n",
    "for wid, pid in unique_pairs:\n",
    "    ts = pdf[(pdf['warehouse_id'] == wid) & (pdf['product_id'] == pid)].sort_values('week_start')\n",
    "    if len(ts) < 10:\n",
    "        continue  # skip very short series\n",
    "    y = ts.set_index('week_start')['weekly_sales']\n",
    "    # Simple train/test split: last 8 weeks as test\n",
    "    train, test = y.iloc[:-8], y.iloc[-8:]\n",
    "    # Fit Exponential Smoothing\n",
    "    try:\n",
    "        model = ExponentialSmoothing(\n",
    "            train, trend='add', seasonal=None, freq=\"W-MON\"\n",
    "        ).fit()\n",
    "        # Store the training date range for predict()\n",
    "        product_train_ranges.setdefault(wid, {})[pid] = (train.index.values[0], train.index.values[-1])\n",
    "        # For metrics, use predict for the test period\n",
    "        pred = model.predict(start=test.index.values[0], end=test.index.values[-1])\n",
    "    except Exception as e:\n",
    "        print(f\"Model failed for warehouse {wid}, product {pid}: {e}\")\n",
    "        continue\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(test, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(test, pred))\n",
    "    mape = mean_absolute_percentage_error(test, pred)\n",
    "    # Store\n",
    "    warehouse_models.setdefault(wid, {})[pid] = model\n",
    "    warehouse_metrics.setdefault(wid, {})[pid] = {'mae': mae, 'rmse': rmse, 'mape': mape}\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(train.index.values, train, label='Train')\n",
    "    ax.plot(test.index.values, test, label='Test', marker='o')\n",
    "    ax.plot(test.index.values, pred, label='Predict', marker='x')\n",
    "    ax.set_title(f'Warehouse {wid}, Product {pid}')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # Save plot to memory\n",
    "    plot_path = f\"tmp/plot_w{wid}_p{pid}.png\"\n",
    "    fig.savefig(plot_path)\n",
    "    plt.close(fig)\n",
    "    warehouse_plots.setdefault(wid, {})[pid] = plot_path\n",
    "\n",
    "print(f\"Trained models for {len(warehouse_models)} warehouses.\")\n",
    "print(\"Sample metrics:\", list(warehouse_metrics.items())[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffd8b8dd-e914-476a-a749-f3d5821105ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Step 3: Define a Custom MLflow Pyfunc Model for Each Warehouse\n",
    "We'll define a custom MLflow Pyfunc class that holds all product models for a warehouse. The class will route predictions to the correct product model based on input product_id. We'll also prepare the model for MLflow logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49057f74-87cc-4695-84d9-ac24e1ec82b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class WarehouseForecastModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, product_models):\n",
    "        \"\"\"\n",
    "        product_models: dict of {product_id: fitted_model}\n",
    "        \"\"\"\n",
    "        self.product_models = product_models\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        model_input: pd.DataFrame with columns ['product_id', 'week_start']\n",
    "        Returns: pd.DataFrame with columns ['product_id', 'week_start', 'prediction']\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for _, row in model_input.iterrows():\n",
    "            pid = row['product_id']\n",
    "            week = pd.to_datetime(row['week_start'])\n",
    "            model = self.product_models.get(pid)\n",
    "            if model is not None:\n",
    "                pred = model.predict(start=week, end=week)\n",
    "                results.append({'product_id': pid, 'week_start': week, 'prediction': float(pred.iloc[0])})\n",
    "            else:\n",
    "                results.append({'product_id': pid, 'week_start': week, 'prediction': None})\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Example usage (not run):\n",
    "# warehouse_model = WarehouseForecastModel(product_models=warehouse_models[wid], product_train_ranges=product_train_ranges[wid])\n",
    "# warehouse_model.predict(None, pd.DataFrame({'product_id': [pid], 'week_start': [pd.Timestamp('2024-01-01')]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ded8c3b-497c-40de-b3de-e8f62fe6c7c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Step 4: Log Models, Plots, and Metrics to MLflow for Each Warehouse\n",
    "For each warehouse, log the custom Pyfunc model to MLflow, including all product models, metrics, and forecast plots as artifacts. We'll use cloudpickle to serialize the product models dictionary. We'll also log metrics and plots for each product as MLflow artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f080c75-37e7-4901-9eb6-eaeb3594275e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if experiment is None:\n",
    "    mlflow.create_experiment(\n",
    "        experiment_name,\n",
    "        artifact_location=f\"dbfs:/Volumes/{catalog}/{schema_forecast}/{ml_artifact_volume}/sales_forecasting/\"\n",
    "    )\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "for wid, product_models in warehouse_models.items():\n",
    "    sample_pid = next(iter(product_models))\n",
    "    example_input = pdf.loc[\n",
    "        (pdf['warehouse_id'] == wid) & (pdf['product_id'] == sample_pid),\n",
    "        ['product_id', 'week_start'],\n",
    "    ].sort_values('week_start')\n",
    "    warehouse_model = WarehouseForecastModel(product_models=product_models)\n",
    "    example_output = warehouse_model.predict(None, example_input)\n",
    "    signature = infer_signature(example_input, example_output)\n",
    "    training_range = product_train_ranges[wid][sample_pid]\n",
    "    training_start = pd.to_datetime(training_range[0]).strftime(\"%Y-%m-%d\")\n",
    "    training_end = pd.to_datetime(training_range[1]).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    run_name = f\"warehouse_{wid}_\" + str(uuid.uuid4())[:8]\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        model_name = f\"warehouse_model_{wid}\"\n",
    "        mlflow.pyfunc.log_model(\n",
    "            name=model_name,\n",
    "            python_model=WarehouseForecastModel(product_models=product_models),\n",
    "            signature=signature,\n",
    "            input_example=example_input,\n",
    "        )\n",
    "        mlflow.log_params({\n",
    "            \"training_start\": training_start,\n",
    "            \"training_end\": training_end,\n",
    "        })\n",
    "        mlflow.set_tag(\"warehouse_id\", wid)\n",
    "        for pid, metrics in warehouse_metrics[wid].items():\n",
    "            mlflow.log_metric(f\"mae_product_{pid}\", metrics['mae'])\n",
    "            mlflow.log_metric(f\"rmse_product_{pid}\", metrics['rmse'])\n",
    "            mlflow.log_metric(f\"mape_product_{pid}\", metrics['mape'])\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            for pid, plot_path in warehouse_plots[wid].items():\n",
    "                dest_path = os.path.join(tmpdir, os.path.basename(plot_path))\n",
    "                # shutil.copy(plot_path, dest_path)\n",
    "                # mlflow.log_artifact(dest_path, artifact_path=f\"plots/product_{pid}\")\n",
    "\n",
    "    print(f\"Logged MLflow run for warehouse {wid}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c68ba62f-28ab-4b01-a3d1-8100cc9fa308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Step 5: Register Models to Unity Catalog under lars_dev.forecast\n",
    "We need to register each warehouse's MLflow model to Unity Catalog under the catalog lars_dev and schema forecast. We'll search for the latest run/model for each warehouse, then use mlflow.register_model with the correct UC path. We'll ensure model names are unique (e.g., warehouse_forecast_{warehouse_id}). We'll also print the registration status for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca0c77e3-3c89-44fa-abf6-4cf45fe6e8ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "# Robustly get experiment_id\n",
    "exp_obj = mlflow.get_experiment_by_name(experiment_name)\n",
    "if exp_obj is None:\n",
    "    raise Exception(f\"Experiment {experiment_name} not found\")\n",
    "experiment_id = exp_obj.experiment_id\n",
    "\n",
    "for wid in warehouse_models.keys():\n",
    "    # Find the latest run for this warehouse\n",
    "    filter_string = f\"tags.warehouse_id = '{wid}'\"\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment_id],\n",
    "        filter_string=filter_string,\n",
    "        order_by=[\"attributes.start_time DESC\"],\n",
    "        max_results=1\n",
    "    )\n",
    "    if not runs:\n",
    "        print(f\"No MLflow run found for warehouse {wid}\")\n",
    "        continue\n",
    "    run = runs[0]\n",
    "    # The model artifact path is warehouse_model_{wid}\n",
    "    model_uri = f\"models:/{run.outputs.model_outputs[0].model_id}\"\n",
    "    model_name = f\"{catalog}.{schema_forecast}.warehouse_forecast_{wid}\"\n",
    "    try:\n",
    "        result = mlflow.register_model(model_uri, model_name)\n",
    "        client.set_registered_model_alias(model_name, \"champion\", result.version)\n",
    "        print(f\"Registered model for warehouse {wid} as {model_name} and set it as champion model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to register model for warehouse {wid}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 Model Training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}