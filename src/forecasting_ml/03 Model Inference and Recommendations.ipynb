{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad708b0-707f-48cb-b544-83efa39a89de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Sales Forecasting and Restocking Recommendations Pipeline\n",
    "\n",
    "This notebook implements an end-to-end workflow for generating sales forecasts and restocking recommendations for each warehouse and product. The process included:\n",
    "\n",
    "1. **Loading Models and Preparing Data:**\n",
    "   * Loaded MLflow models for each warehouse.\n",
    "   * Read historical sales data and identified all (warehouse_id, product_id) pairs and week_start dates.\n",
    "2. **Generating Forecasts:**\n",
    "   * Produced forecasts for all historical and 6 future weeks for each warehouse/product using the loaded models.\n",
    "   * Wrote the results to the `lars_dev.forecast.sales_forecast` table.\n",
    "3. **Restocking Recommendations:**\n",
    "   * Sampled and explored both the forecast and inventory tables to confirm schemas.\n",
    "   * Calculated the sum of forecasted sales for the next 4 weeks and compared it to the most recent inventory levels.\n",
    "   * Recommended restocking for any (warehouse_id, product_id) where inventory is predicted to run out in the next 4 weeks.\n",
    "   * Wrote 71 restocking recommendations to the `lars_dev.forecast.inventory_forecast` table, including columns: warehouse_id, product_id, inventory_level, forecast_4w, restock_qty.\n",
    "\n",
    "**Next Steps:**\n",
    "* Review the `inventory_forecast` table for actionable restocking decisions.\n",
    "* Adjust forecast horizon or restocking logic as needed for your business requirements.\n",
    "\n",
    "_This pipeline is ready for productionization and further automation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a2af91-5005-4472-889b-08819e914893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow-skinny=3.3.2\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96fe168e-b4d9-41e1-94b1-de9f1a859d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Load Models and Prepare Data\n",
    "Load trained models for each warehouse from MLflow and read historical sales data. Identify all unique (warehouse_id, product_id) pairs and the full range of historical week_start dates for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b739b756-1966-4d4e-a138-d78a1e45a20a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType, DoubleType\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e431030-b56d-404e-b5ca-c8a074a5324d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"smart_stock\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "dbutils.widgets.text(\"schema_silver\", \"silver\")\n",
    "schema_silver = dbutils.widgets.get(\"schema_silver\")\n",
    "dbutils.widgets.text(\"schema_forecast\", \"forecast\")\n",
    "schema_forecast = dbutils.widgets.get(\"schema_forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "276091b5-ec1f-4fcc-bfb1-1177332d8da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read historical sales data (limit to last 3 years for efficiency)\n",
    "sales_history = spark.read.table(f\"{catalog}.{schema_silver}.sales_history\") \\\n",
    "    .filter(F.col('week_start') >= F.date_sub(F.current_date(), 7*52*3))\n",
    "\n",
    "# Get unique (warehouse_id, product_id) pairs and all week_start dates\n",
    "unique_pairs = sales_history.select('warehouse_id', 'product_id').distinct().toPandas()\n",
    "unique_warehouse_ids = unique_pairs['warehouse_id'].unique()\n",
    "all_weeks = sales_history.select('week_start').distinct().toPandas()['week_start'].sort_values().tolist()\n",
    "\n",
    "# Prepare a dictionary to hold loaded models for each warehouse\n",
    "warehouse_models = {}\n",
    "\n",
    "# Load Unity Catalog model with alias 'champion' for each warehouse\n",
    "for wid in unique_pairs['warehouse_id'].unique():\n",
    "    model_name = f\"{catalog}.{schema_forecast}.warehouse_forecast_{wid}\"\n",
    "    try:\n",
    "        warehouse_models[wid] = mlflow.pyfunc.load_model(f\"models:/{model_name}@champion\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not warehouse_models:\n",
    "    raise ValueError(\"No models loaded. Check model registry and warehouse IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5d9f57c-be7e-46e8-bd77-aab4792b6cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Generate Forecasts for All Historical Dates Plus 6 Weeks Ahead\n",
    "For each warehouse and product, generate predictions for all historical week_start dates plus 6 future weeks. Assemble a DataFrame with columns: warehouse_id, product_id, week_start, forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b00eec3-32d2-4e1d-a365-1dede55ddc3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare forecast horizon: all historical weeks + 6 future weeks\n",
    "df_weeks = pd.DataFrame({'week_start': all_weeks})\n",
    "last_week = pd.to_datetime(df_weeks['week_start']).max()\n",
    "future_weeks = [last_week + timedelta(weeks=i) for i in range(1, 7)]\n",
    "all_forecast_weeks = pd.to_datetime(df_weeks['week_start']).tolist() + future_weeks\n",
    "\n",
    "# Prepare forecast input DataFrame for each warehouse\n",
    "forecast_results = []\n",
    "warehouse_inputs = {\n",
    "    wid: { \"product_id\": [], \"week_start\": [] }\n",
    "    for wid in unique_warehouse_ids\n",
    "}\n",
    "for row in unique_pairs.itertuples():\n",
    "    wid = row.warehouse_id\n",
    "    pid = row.product_id\n",
    "    warehouse_inputs[wid]['product_id'].extend([pid]*len(all_forecast_weeks))\n",
    "    warehouse_inputs[wid]['week_start'].extend(all_forecast_weeks)\n",
    "\n",
    "for wid in unique_warehouse_ids:\n",
    "    model = warehouse_models.get(wid)\n",
    "    if model is None:\n",
    "        continue\n",
    "    # Prepare input DataFrame for this warehouse\n",
    "    input_df = pd.DataFrame(warehouse_inputs[wid])\n",
    "    # Predict\n",
    "    try:\n",
    "        preds = model.predict(input_df)\n",
    "        preds['warehouse_id'] = wid\n",
    "        forecast_results.append(preds)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not forecast_results:\n",
    "    raise ValueError(\"No forecast results were generated. Check model loading and prediction logic.\")\n",
    "\n",
    "# Combine all forecasts into a single DataFrame\n",
    "forecast_df = pd.concat(forecast_results, ignore_index=True)\n",
    "forecast_df = forecast_df[['warehouse_id', 'product_id', 'week_start', 'prediction']].rename(columns={'prediction': 'forecast'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a05b8f49-dc93-4eef-9f5d-ec61475c5bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Step 3: Write Forecasts to Table\n",
    "Convert the pandas DataFrame to a Spark DataFrame, ensure correct types, and write to the target table, overwriting existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78623cd-eb6c-45fa-a7ba-211f42a70c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure correct dtypes in pandas DataFrame\n",
    "df = forecast_df.copy()\n",
    "df['warehouse_id'] = df['warehouse_id'].astype(int)\n",
    "df['product_id'] = df['product_id'].astype(int)\n",
    "df['forecast'] = df['forecast'].astype(float)\n",
    "df['week_start'] = pd.to_datetime(df['week_start'])\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Ensure week_start is timestamp\n",
    "target_df = spark_df.withColumn('week_start', F.col('week_start').cast(TimestampType()))\n",
    "\n",
    "# Write to table, overwrite mode\n",
    "target_df.write.mode('overwrite').saveAsTable(f\"{catalog}.{schema_forecast}.sales_forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2da2ebbe-c62f-4e18-92cf-cfb0f6769c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Calculate Restocking Recommendations\n",
    "Join the most recent inventory levels with the 4-week forecast sums, and recommend restocking if inventory will run out. Prepare the result for writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc201a41-bee4-445b-8c71-f8b19fb7037f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Get most recent inventory_level for each (warehouse_id, product_id)\n",
    "inv = spark.read.table(f\"{catalog}.{schema_silver}.inventory_history\")\n",
    "\n",
    "window = Window.partitionBy('warehouse_id', 'product_id').orderBy(F.col('date').desc())\n",
    "latest_inv = inv.withColumn('rn', F.row_number().over(window)) \\\n",
    "    .filter(F.col('rn') == 1) \\\n",
    "    .select('warehouse_id', 'product_id', F.col('inventory_level').alias('current_stock'))\n",
    "\n",
    "# 2. Sum forecast for next 30 days for each (warehouse_id, product_id)\n",
    "forecast = spark.read.table(f\"{catalog}.{schema_forecast}.sales_forecast\")\n",
    "future_30d = forecast \\\n",
    "    .filter(F.col('week_start') >= F.current_date()) \\\n",
    "    .filter(F.col('week_start') < F.date_add(F.current_date(), 30)) \\\n",
    "    .groupBy('warehouse_id', 'product_id') \\\n",
    "    .agg(F.sum('forecast').alias('forecast_30_days'))\n",
    "\n",
    "# 3. Join and calculate reorder logic\n",
    "rec = latest_inv.join(future_30d, ['warehouse_id', 'product_id'], 'inner')\n",
    "\n",
    "# Set reorder_quantity as forecast_30_days - current_stock if needed\n",
    "rec = rec.withColumn('reorder_quantity', F.when(F.col('current_stock') < F.col('forecast_30_days'), F.col('forecast_30_days') - F.col('current_stock')).otherwise(F.lit(0)))\n",
    "rec = rec.filter(F.col('reorder_quantity') > 0)\n",
    "\n",
    "# Confidence score: high when recommendation is clear (either definitely need to reorder or definitely don't)\n",
    "def confidence_udf(current_stock, forecast_30_days):\n",
    "    if forecast_30_days <= 0:\n",
    "        return 0.9  # High confidence that no reorder is needed if no forecast demand\n",
    "    \n",
    "    ratio = current_stock / forecast_30_days\n",
    "    \n",
    "    if ratio <= 0.5:\n",
    "        # Very low stock relative to forecast - high confidence TO reorder\n",
    "        return 1.0\n",
    "    elif ratio <= 1.0:\n",
    "        # Stock will run out - high confidence TO reorder, scaling from 0.8 to 1.0\n",
    "        return 0.8 + 0.2 * (1.0 - ratio) / 0.5\n",
    "    elif ratio <= 1.5:\n",
    "        # Uncertain zone - medium confidence, scaling from 0.4 to 0.8\n",
    "        return 0.4 + 0.4 * abs(1.25 - ratio) / 0.25\n",
    "    elif ratio <= 2.5:\n",
    "        # Good buffer - increasing confidence NOT to reorder, scaling from 0.6 to 0.9\n",
    "        return 0.6 + 0.3 * (ratio - 1.5) / 1.0\n",
    "    else:\n",
    "        # Plenty of stock - high confidence NOT to reorder\n",
    "        return 0.95\n",
    "\n",
    "confidence_score_udf = udf(confidence_udf, DoubleType())\n",
    "rec = rec.withColumn('confidence_score', confidence_score_udf(F.col('current_stock'), F.col('forecast_30_days')))\n",
    "rec = rec.join(\n",
    "    spark.read.table(f\"{catalog}.{schema_silver}.dim_products\")\n",
    "        .select('product_id', F.col('reorder_level').alias('reorder_point')),\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select and order columns as per schema\n",
    "rec_final = rec.select(\n",
    "    'product_id',\n",
    "    'warehouse_id',\n",
    "    'current_stock',\n",
    "    'forecast_30_days',\n",
    "    'reorder_point',\n",
    "    'reorder_quantity',\n",
    "    F.round('confidence_score', 2).alias('confidence_score')\n",
    ")\n",
    "\n",
    "display(rec_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf39b3d8-2def-42e8-82a6-f806eef30868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Write Restocking Recommendations to Table\n",
    "Write the restocking recommendations DataFrame to the lars_dev.forecast.inventory_forecast table, overwriting any existing data. This will make the recommendations available for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91de38b1-83b7-414e-8c98-2fbe249c7f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write recommendations to lars_dev.forecast.inventory_forecast (overwrite mode)\n",
    "rec_final.write.mode('overwrite').saveAsTable(f\"{catalog}.{schema_forecast}.inventory_forecast\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 Model Inference and Recommendations",
   "widgets": {
    "catalog": {
     "currentValue": "smart_stock",
     "nuid": "776efe0b-56ae-46b2-8a6a-ab23be7121d4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "smart_stock",
      "label": null,
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "smart_stock",
      "label": null,
      "name": "catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schema_forecast": {
     "currentValue": "forecast",
     "nuid": "b79aa383-a05a-4cc2-8813-065e5c962b87",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "forecast",
      "label": null,
      "name": "schema_forecast",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "forecast",
      "label": null,
      "name": "schema_forecast",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schema_silver": {
     "currentValue": "silver",
     "nuid": "99c0e936-1fad-45d0-8732-2242ac53d4b3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "silver",
      "label": null,
      "name": "schema_silver",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "silver",
      "label": null,
      "name": "schema_silver",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
